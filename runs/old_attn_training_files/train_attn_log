nohup: ignoring input
/scratch/miniconda3/envs/roko/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /scratch/model/roko_rnn exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
wandb: Currently logged in as: ws500981 (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.1
wandb: Syncing run genial-paper-9
wandb:  View project at https://wandb.ai/ws500981/docker_roko
wandb:  View run at https://wandb.ai/ws500981/docker_roko/runs/osa2okk7
wandb: Run data is saved locally in /scratch/roko/sequence-polishing/runs/wandb/run-20211018_075127-osa2okk7
wandb: Run `wandb offline` to turn off syncing.
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name           | Type      | Params
---------------------------------------------
0 | embedding      | Embedding | 1.5 K 
1 | do             | Dropout   | 0     
2 | evoformer      | Evoformer | 2.4 M 
3 | train_accuracy | Accuracy  | 0     
4 | val_accuracy   | Accuracy  | 0     
5 | fc4            | Linear    | 645   
---------------------------------------------
2.4 M     Trainable params
0         Non-trainable params
2.4 M     Total params
9.507     Total estimated model params size (MB)
/scratch/miniconda3/envs/roko/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/scratch/miniconda3/envs/roko/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb: Waiting for W&B process to finish, PID 33394
wandb: Program ended successfully.
wandb: - 17.29MB of 17.29MB uploaded (0.00MB deduped)wandb: \ 17.29MB of 17.29MB uploaded (0.00MB deduped)wandb: | 17.29MB of 17.29MB uploaded (0.00MB deduped)wandb: / 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.29MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.52MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.66MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: | 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: / 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: - 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb: \ 17.72MB of 17.72MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /scratch/roko/sequence-polishing/runs/wandb/run-20211018_075127-osa2okk7/logs/debug.log
wandb: Find internal logs for this run at: /scratch/roko/sequence-polishing/runs/wandb/run-20211018_075127-osa2okk7/logs/debug-internal.log
wandb: Run summary:
wandb:                                                                train_loss 0.00015
wandb:                                       grad_2.0_norm_embedding.weight_step 0.0002
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.weight_step 0.0004
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.weight_step 0.0006
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.bias_step 0.0001
wandb:    grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.weight_step 0.0003
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.weight_step 0.0003
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.bias_step 0.0001
wandb:    grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.weight_step 0.0003
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.weight_step 0.0002
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.weight_step 0.0003
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.weight_step 0.0
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.weight_step 0.0004
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.weight_step 0.0004
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.weight_step 0.0004
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.bias_step 0.0001
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.weight_step 0.0
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.weight_step 0.0005
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.bias_step 0.0
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.weight_step 0.0001
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.bias_step 0.0
wandb:    grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.weight_step 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.bias_step 0.0001
wandb:                                             grad_2.0_norm_fc4.weight_step 0.025
wandb:                                               grad_2.0_norm_fc4.bias_step 0.0001
wandb:                                                  grad_2.0_norm_total_step 0.025
wandb:                                                                     epoch 30
wandb:                                                       trainer/global_step 3099
wandb:                                                                  _runtime 6881
wandb:                                                                _timestamp 1634550368
wandb:                                                                     _step 123
wandb:                                                             val_acc_batch 0.99857
wandb:                                                            val_loss_batch 0.01797
wandb:                                                             val_acc_epoch 0.99802
wandb:                                      grad_2.0_norm_embedding.weight_epoch 0.00079
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.weight_epoch 0.00036
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.bias_epoch 0.00035
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.weight_epoch 0.00026
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.bias_epoch 0.00023
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.weight_epoch 0.00139
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.bias_epoch 0.00011
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.weight_epoch 0.00234
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.bias_epoch 0.00023
wandb:   grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.weight_epoch 0.00011
wandb:     grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.bias_epoch 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.weight_epoch 0.00017
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.bias_epoch 0.00018
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.weight_epoch 0.00016
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.bias_epoch 0.00017
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.weight_epoch 0.00093
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.bias_epoch 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.weight_epoch 0.00157
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.bias_epoch 0.00017
wandb:   grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.weight_epoch 0.0001
wandb:     grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.bias_epoch 0.0001
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.weight_epoch 0.00011
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.bias_epoch 0.00012
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.weight_epoch 0.00011
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.bias_epoch 0.00012
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.weight_epoch 0.00072
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.weight_epoch 0.00113
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.bias_epoch 0.00012
wandb:   grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.weight_epoch 6e-05
wandb:     grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.weight_epoch 0.00011
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.bias_epoch 0.00011
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.weight_epoch 0.0001
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.bias_epoch 0.00011
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.weight_epoch 0.0006
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.weight_epoch 0.00084
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.bias_epoch 0.00011
wandb:   grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.weight_epoch 6e-05
wandb:     grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.bias_epoch 5e-05
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.weight_epoch 0.0005
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.weight_epoch 0.00074
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.bias_epoch 6e-05
wandb:   grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.weight_epoch 6e-05
wandb:     grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.bias_epoch 5e-05
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.weight_epoch 0.00044
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.bias_epoch 5e-05
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.weight_epoch 0.00067
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.bias_epoch 6e-05
wandb:   grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.weight_epoch 5e-05
wandb:     grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.bias_epoch 4e-05
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.weight_epoch 0.00039
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.bias_epoch 4e-05
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.weight_epoch 0.00057
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.bias_epoch 6e-05
wandb:   grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.weight_epoch 5e-05
wandb:     grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.bias_epoch 4e-05
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.weight_epoch 6e-05
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.bias_epoch 6e-05
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.weight_epoch 0.00034
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.bias_epoch 4e-05
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.weight_epoch 0.00055
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.bias_epoch 6e-05
wandb:   grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.weight_epoch 4e-05
wandb:     grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.bias_epoch 4e-05
wandb:                                            grad_2.0_norm_fc4.weight_epoch 0.01976
wandb:                                              grad_2.0_norm_fc4.bias_epoch 0.00011
wandb:                                                 grad_2.0_norm_total_epoch 0.02019
wandb:                                                           Train_acc_epoch 0.99928
wandb: Run history:
wandb:                                                                train_loss ▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▄▁▁▁▁▁▅▁▁▁▁█▇▁▁▁▁▁▁▁▁▁▁▁
wandb:                                       grad_2.0_norm_embedding.weight_step ▁▇▅▁▅▂▂▃▅█▃▂▅▂▁▂▂▄▄▁▂▃▃▂▁▃▂▃▃▂▂▃▄▂▂▁▂▁▂▂
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.weight_step ▁█▅▁▄▂▂▂▄▆▂▂▄▂▁▂▂▃▄▁▂▃▂▂▁▂▂▃▃▂▂▃▃▂▂▁▂▁▂▂
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.bias_step ▁▇▆▁▆▂▃▂▅█▃▂▇▂▁▃▂▅▃▂▁▄▃▃▁▃▂▃▄▂▂▃▅▂▂▁▂▂▂▂
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.weight_step ▁█▅▁▄▂▂▂▄▇▂▁▄▂▁▂▂▂▄▁▁▂▂▂▁▂▂▂▂▂▂▄▂▂▂▁▁▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.bias_step ▁▇▅▁▅▁▂▂▅█▄▂▇▂▁▂▂▅▂▁▁▄▂▂▁▄▂▄▄▂▂▄▅▂▂▁▂▂▂▂
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.weight_step ▁▅▃▁▄▂▂▃▅█▃▂▅▂▁▂▂▄▅▂▂▄▃▃▁▃▂▃▄▂▂▃▄▂▂▁▂▁▂▂
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.bias_step ▁▅▃▁▅▁▃▃▅█▃▁▅▃▁▃▃▅▅▁▁▅▃▃▁▃▃▃▅▃▃▃▅▃▃▁▁▁▃▁
wandb:       grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.weight_step ▁▆▅▁▅▂▂▃▅█▃▂▆▂▁▂▂▅▄▁▂▃▂▃▁▃▂▃▃▂▂▃▄▂▂▁▂▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.bias_step ▁▆▅▁▅▁▂▂▅█▃▂▆▂▁▂▂▅▂▂▁▃▂▃▁▃▂▃▅▂▂▃▅▂▂▁▂▂▂▂
wandb:    grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.weight_step ▁▃▂▁▃▂▂▂▆█▃▂▅▂▁▂▂▅▅▁▂▃▃▃▁▃▂▃▃▂▂▃▅▂▂▁▂▁▂▂
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.bias_step ▁▅▃▁▅▁▃▃▅█▅▃▆▃▁▃▃▆▃▃▁▅▃▅▁▅▅▅▅▃▃▅▆▃▃▁▃▁▃▃
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.weight_step ▁█▆▁▆▃▃▃▆█▃▁▆▃▁▃▃▃▆▁▁▃▃▃▁▃▃▃▃▃▃▃▃▃▁▁▁▁▃▁
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.bias_step ▁▆▆▁▆▁▃▃▅█▃▃▆▃▁▃▃▆▃▁▁▅▃▃▁▅▃▃▅▃▃▃▅▁▃▁▁▃▃▃
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.weight_step ▁█▆▁▆▃▁▃▆█▃▁▃▁▁▁▁▃▆▁▁▃▃▃▁▃▃▃▃▁▁▃▃▁▁▁▁▁▃▁
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.bias_step ▁▆▅▁▅▁▃▃▅█▃▃▆▃▁▃▃▅▃▁▁▃▃▃▁▃▃▃▅▃▃▃▅▁▁▁▁▁▃▁
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.weight_step ▁▅▃▁▄▂▂▂▆█▄▂▅▂▁▂▂▄▅▂▂▃▃▃▁▃▃▃▄▂▂▄▄▂▂▁▂▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.bias_step ▁█▅▁▅▁▅▁██▅▁█▅▁▅▅▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅█▁▅▁▁▁▅▁
wandb:       grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.weight_step ▁▆▅▁▅▂▂▂▅█▃▂▅▂▁▂▂▄▄▁▁▃▂▂▁▃▂▃▃▂▂▃▄▂▂▁▁▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.bias_step ▁▆▅▁▅▁▃▃▅█▃▃▆▃▁▃▃▅▃▁▁▅▃▃▁▃▃▃▅▃▃▃▅▁▃▁▁▁▃▃
wandb:    grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.weight_step ▁▅▃▁▃▃▃▃▆█▅▃▅▃▁▃▃▅▆▁▃▃▃▃▁▅▃▅▅▃▃▅▅▃▃▁▃▁▃▃
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.bias_step ▁▆▃▁▃▁▃▁▆█▆▁▆▃▁▃▃▆▃▃▁▃▃▃▁▆▃▆▆▃▃▆█▃▃▁▃▁▃▃
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.weight_step ▁▆▃▁▃▁▁▃▆█▃▁▃▃▁▃▁▃▆▁▁▃▃▃▁▃▃▃▃▁▁▃▃▁▁▁▁▁▃▁
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.bias_step ▁▆▆▁▆▁▃▃▆█▃▁▆▃▁▃▃▆▃▁▁▃▃▃▁▃▃▃▆▃▃▃▆▁▁▁▁▁▃▁
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.weight_step ▁█▅▁▅▁▁▅▅█▅▁▅▁▁▁▁▅█▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.bias_step ▁▆▆▁▆▁▃▁▃█▃▁▆▃▁▃▃▆▃▁▁▃▃▃▁▃▃▃▃▃▁▃▆▁▁▁▁▁▃▁
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.weight_step ▁▄▃▁▃▂▂▂▅█▃▂▅▂▁▂▂▄▅▂▂▃▃▃▁▄▃▃▅▂▂▄▅▃▂▁▂▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▅▁▁▅▅▅▁▅▅▅█▅▅▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.weight_step ▁▆▅▁▅▂▂▃▅█▄▂▅▂▁▂▂▄▄▂▂▃▃▃▁▃▂▃▄▂▂▃▄▂▂▁▂▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.bias_step ▁▆▆▁▆▁▃▁▆█▃▁▆▃▁▃▃▆▃▁▁▃▃▃▁▃▃▃▆▃▃▃▆▁▃▁▁▁▃▁
wandb:    grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.weight_step ▁▃▃▁▃▁▃▃▆█▅▃▅▃▁▃▃▅▆▃▃▃▃▃▁▅▃▅▅▃▃▆▆▃▃▁▃▁▃▃
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.bias_step ▁▃▃▁▃▁▃▁▆█▃▃▆▃▁▃▃▆▃▃▁▃▃▃▁▆▃▆█▃▃▆█▃▃▁▁▃▃▃
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.weight_step ▁▆▃▁▃▁▁▃▆█▃▁▃▁▁▁▁▃▆▁▁▃▃▃▁▃▃▃▃▁▁▃▃▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.bias_step ▁▆▃▁▃▁▃▁▃█▃▁▆▃▁▃▃▆▁▁▁▃▃▃▁▃▃▃▃▃▃▃▆▁▁▁▁▁▃▁
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.weight_step ▁█▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.bias_step ▁█▅▁▅▁▅▁▅█▅▁█▅▁▅▅█▁▁▁▅▅▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.weight_step ▁▄▃▁▃▂▂▂▅█▃▂▅▂▁▂▂▄▅▂▂▃▃▃▁▄▃▄▅▂▂▄▅▃▂▁▂▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.bias_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▅▁▁▅▅▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.weight_step ▁▆▄▁▅▂▂▂▅█▃▂▅▂▁▂▂▄▄▂▂▃▃▃▁▃▂▃▄▂▂▃▄▂▂▁▂▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.bias_step ▁▆▃▁▃▁▃▁▃█▃▁▆▃▁▃▃▆▁▁▁▃▃▃▁▃▃▃▆▃▃▃▆▁▁▁▁▁▃▁
wandb:    grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.weight_step ▁▃▃▁▃▁▁▁▆█▃▃▆▃▁▃▃▆▆▁▁▃▃▃▁▆▃▆▆▃▃▆█▃▃▁▃▁▃▁
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.bias_step ▁▃▁▁▃▁▃▁▃█▃▁▆▃▁▃▃▆▃▃▁▃▃▃▁▆▃▆▆▃▃▆█▃▃▁▃▃▃▃
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.bias_step ▁▅▅▁▅▁▅▁▅█▅▁█▅▁▅▅█▁▁▁▅▅▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.bias_step ▁▅▅▁▅▁▅▁▅█▅▁█▅▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▁▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.weight_step ▁▄▃▁▄▂▂▂▅█▃▂▅▂▁▂▂▅▅▂▂▄▃▃▁▄▃▄▅▂▂▅▆▃▂▁▂▂▃▂
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.bias_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▅▁▁▅▅▅▁▅▅▅▅▅▁▅█▁▅▁▁▁▅▁
wandb:       grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.weight_step ▁▅▄▁▄▂▂▂▅█▄▂▅▂▁▂▂▄▄▂▂▃▃▃▁▃▂▃▄▂▂▃▄▂▂▁▂▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.bias_step ▁▅▅▁▅▁▅▁▅█▅▁█▅▁▅▅█▁▁▁▅▅▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:    grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.weight_step ▁▃▃▁▃▁▁▃▅█▃▃▅▃▁▃▃▅▅▃▁▃▃▃▁▅▃▅▅▃▃▅▆▃▃▁▃▁▃▃
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.bias_step ▁▃▁▁▃▁▃▁▃█▃▃▆▃▁▃▃▆▃▃▁▃▃▆▁▆▃▆▆▃▃▆█▃▃▁▃▃▃▃
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▅▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.bias_step ▁▅▅▁▅▁▅▁▅█▅▁█▅▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▁▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.weight_step ▁▃▃▁▃▂▂▂▅█▃▂▅▂▁▃▃▅▆▂▂▃▃▃▁▅▃▄▆▃▃▆▆▃▃▂▃▂▃▂
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.bias_step ▁▅▁▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▅▁▁▅▅▅▁▅▅▅▅▅▅▅█▅▅▁▁▁▅▁
wandb:       grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.weight_step ▁▆▅▁▅▂▂▃▅█▃▂▅▂▁▃▂▄▄▂▁▃▃▃▁▃▂▃▄▂▂▃▄▂▂▁▂▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.bias_step ▁▅▅▁▅▁▅▁▅█▅▁█▅▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:    grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.weight_step ▁▃▁▁▃▁▁▁▆█▃▃▆▃▁▃▃▆█▃▃▃▃▃▁█▃▆▆▃▃██▃▃▁▃▁▃▃
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.bias_step ▁▃▁▁▃▁▃▁▃▆▃▃▅▃▁▃▃▅▃▃▁▃▃▅▁▆▃▅▆▃▃▆█▃▃▁▃▃▃▃
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▅▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▁▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.bias_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▁▁▁▅▁▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.weight_step ▁▃▂▁▃▂▂▂▅█▄▂▅▂▁▃▃▅▆▂▂▃▃▃▁▅▃▅▆▃▃▆▇▃▃▂▃▂▂▂
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.bias_step ▁▅▁▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▅▁▁▅▅▅▁▅▅▅█▅▅▅█▁▅▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.weight_step ▁▅▄▁▄▂▂▂▅█▃▂▅▂▁▂▂▄▄▂▁▃▃▃▁▃▃▃▄▂▂▃▄▂▂▁▂▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▁▁▁▅▅▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:    grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.weight_step ▁▃▁▁▃▁▁▁▅▆▅▃▅▃▁▃▃▅▅▃▃▃▃▃▁▆▃▅▆▃▃▆█▃▃▁▃▃▃▃
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.bias_step ▁▂▁▁▂▁▂▁▂▅▂▂▄▂▁▂▂▄▂▂▁▂▂▄▁▅▄▄▅▂▂▅█▂▂▁▂▂▂▂
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▅▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▁▁▁▅▁▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.weight_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▁▁▅▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅▅▁▁▁▁▁▁
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.bias_step ▁▅▅▁▅▁▁▁▅█▅▁▅▁▁▅▅▅▁▁▁▅▁▅▁▅▅▅▅▅▁▅█▁▁▁▁▁▁▁
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.weight_step ▁▃▂▁▃▂▂▂▅█▄▂▆▂▁▃▃▅▆▂▂▄▃▄▁▆▃▅▆▃▃▆█▄▃▂▃▂▃▃
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.bias_step ▁▅▁▁▅▁▁▁▅█▅▁▅▁▁▅▁▅▅▁▁▅▅▅▁▅▅▅▅▅▅▅█▅▅▁▁▁▅▁
wandb:       grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.weight_step ▁▆▄▁▄▂▂▂▅█▃▂▅▂▁▂▂▄▄▂▁▃▃▃▁▄▂▃▄▂▂▄▄▂▂▁▂▁▂▁
wandb:         grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.bias_step ▁▅▅▁▅▁▁▁▅█▅▁█▁▁▅▅▅▁▁▁▅▁▅▁▅▅▅▅▅▅▅█▁▁▁▁▁▁▁
wandb:    grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.weight_step ▁▃▁▁▃▁▁▁▅▆▃▃▅▃▁▃▃▅▅▃▁▃▃▃▁▆▃▅▅▃▃▆█▃▃▁▃▃▃▃
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.bias_step ▁▃▁▁▃▁▁▁▃▅▃▃▅▃▁▃▃▅▃▃▁▃▃▅▁▆▃▅▅▃▃▅█▃▃▁▃▃▃▃
wandb:                                             grad_2.0_norm_fc4.weight_step ▁▄▃▁▄▂▂▂▅█▃▂▆▂▁▃▃▆▆▂▂▄▃▃▁▅▃▄▅▃▃▅▇▃▃▂▃▂▃▃
wandb:                                               grad_2.0_norm_fc4.bias_step ▁▅▃▁▅▁▃▃▅█▃▃▆▃▁▃▃▆▃▃▁▅▃▅▁▆▃▅▆▃▃▆█▃▃▁▃▃▅▃
wandb:                                                  grad_2.0_norm_total_step ▁▄▃▁▄▂▂▂▅█▃▂▆▂▁▃▃▆▆▂▂▄▃▃▁▅▃▄▅▃▃▅▇▃▃▂▃▂▃▃
wandb:                                                                     epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:                                                       trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:                                                                  _runtime ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇████
wandb:                                                                _timestamp ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇████
wandb:                                                                     _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                             val_acc_batch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                                            val_loss_batch █▆▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁
wandb:                                                             val_acc_epoch ▁▅▆▆▇▇▇▇▇██████████████████████
wandb:                                      grad_2.0_norm_embedding.weight_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.weight_epoch ▁█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_row_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.weight_epoch ▁█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_col_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_transition.linear1.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.weight_epoch ▁██████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.0.msa_transition.linear2.bias_epoch ▁██████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.0.msa_transition.layer_norm.bias_epoch ▁▁█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.weight_epoch ▁██████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_row_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.weight_epoch ▁███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_col_att.layer_norm.bias_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_transition.linear1.bias_epoch ███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.weight_epoch ▁██████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.1.msa_transition.linear2.bias_epoch ▁██████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.weight_epoch ▁▁█████████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.1.msa_transition.layer_norm.bias_epoch ███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.weight_epoch ▁▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_row_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.weight_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_col_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_transition.linear1.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.2.msa_transition.linear2.bias_epoch ▁██████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.2.msa_transition.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.weight_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_row_att.layer_norm.bias_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.weight_epoch ███▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_col_att.layer_norm.bias_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_transition.linear1.bias_epoch ▁███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.3.msa_transition.linear2.bias_epoch ▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:   grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.3.msa_transition.layer_norm.bias_epoch ▁▁▁▁▁▁▁████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_row_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_col_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.weight_epoch ▁▆█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_transition.linear1.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.4.msa_transition.linear2.bias_epoch ▁▇█████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.4.msa_transition.layer_norm.bias_epoch ▁▁▁▁█▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_row_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_col_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.weight_epoch ▁▆█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_transition.linear1.bias_epoch ████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.weight_epoch ▁██████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.5.msa_transition.linear2.bias_epoch ▁▇█████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.weight_epoch ▁▁▁▁▁▁▅████████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.5.msa_transition.layer_norm.bias_epoch █▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_row_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_col_att.layer_norm.bias_epoch ▁▇█████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.weight_epoch ▁▆█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_transition.linear1.bias_epoch █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.6.msa_transition.linear2.bias_epoch ▁▇█████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.weight_epoch ▁▁▁▁▁▁▄▇███████████████████████
wandb:     grad_2.0_norm_evoformer.blocks.6.msa_transition.layer_norm.bias_epoch █▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.weight_epoch ▁▇█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_row_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.weight_epoch ▁███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_col_att.layer_norm.bias_epoch ▁██████████████████████████████
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.weight_epoch ▁▆█████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_transition.linear1.bias_epoch █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.weight_epoch ▁██████████████████████████████
wandb:        grad_2.0_norm_evoformer.blocks.7.msa_transition.linear2.bias_epoch ▁██████████████████████████████
wandb:   grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.weight_epoch █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     grad_2.0_norm_evoformer.blocks.7.msa_transition.layer_norm.bias_epoch █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                            grad_2.0_norm_fc4.weight_epoch ▁▇█████████████████████████████
wandb:                                              grad_2.0_norm_fc4.bias_epoch ▁▇█████████████████████████████
wandb:                                                 grad_2.0_norm_total_epoch ▁▇█████████████████████████████
wandb:                                                           Train_acc_epoch ▁▅▆▆▇▇▇▇▇██████████████████████
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced genial-paper-9: https://wandb.ai/ws500981/docker_roko/runs/osa2okk7
